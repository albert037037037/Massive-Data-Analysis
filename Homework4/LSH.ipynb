{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf, SparkContext\n",
    "import binascii"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mapper_split(s):\n",
    "    str_split = []\n",
    "    str_return = []\n",
    "    str_split = s.split()\n",
    "    for word in str_split:\n",
    "        if word != \"\" and ((ord(word[0]) >= 48 and ord(word[0]) <= 57) or (ord(word[0]) >= 65 and ord(word[0]) <= 90) or (ord(word[0]) >= 97 and ord(word[0]) <= 122)):\n",
    "            str_return.append(word)\n",
    "    return (0, str_return)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mapper_combine3(line):\n",
    "    shingle = []\n",
    "    shingle_set = []\n",
    "    tmp = \"\"\n",
    "    for i in range(len(line[1])):\n",
    "        tmp = line[1][i] + ' ' + line[1][i+1] + ' ' + line[1][i+2]\n",
    "        tmp = binascii.crc32(tmp.encode('utf8'))\n",
    "        shingle.append(tmp)\n",
    "        if i == len(line[1])-3:\n",
    "            break\n",
    "    for i in range(len(shingle)):\n",
    "        if shingle[i] not in shingle_set:\n",
    "            shingle_set.append(shingle[i])\n",
    "    return shingle_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mapper_into_all_index(line): # to find the index in All_shingle\n",
    "    index_list = []\n",
    "    for i in range(1,len(line)):\n",
    "        index = [All_shingle.index(line[i])]  # Need to turn the index turn to list type to make it an element of list\n",
    "        index_list.append((line[0], index))\n",
    "    return index_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [0] * 102\n",
    "All_shingle = []\n",
    "Shingle_docs = [0] * 101\n",
    "All_shingle_len = 0\n",
    "for i in range(1, 102):\n",
    "    if i < 10:\n",
    "        data[i] = sc.textFile(\"athletics/00\"+str(i)+\".txt\")\n",
    "    elif i < 100:\n",
    "        data[i] = sc.textFile(\"athletics/0\"+str(i)+\".txt\")\n",
    "    elif i < 102:\n",
    "        data[i] = sc.textFile(\"athletics/\"+str(i)+\".txt\")\n",
    "    data[i] = data[i].map(mapper_split).reduceByKey(lambda a, b: a+b).flatMap(mapper_combine3)\n",
    "    doc = data[i].collect()\n",
    "    for word in doc:\n",
    "        if word not in All_shingle:\n",
    "            All_shingle.append(word)\n",
    "    doc.append(i)\n",
    "    doc.sort()\n",
    "    Shingle_docs[i-1] = doc\n",
    "All_shingle.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change every shingle doc into rdd\n",
    "Shingle_docs_RDD = sc.parallelize(Shingle_docs)\n",
    "Shingle_docs_RDD = Shingle_docs_RDD.flatMap(mapper_into_all_index).reduceByKey(lambda a, b: a+b).sortByKey(ascending = True)\n",
    "#print(Shingle_docs_RDD.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mapper_hash(line):\n",
    "    tmp = [All_shingle_len+1] * 100\n",
    "    real_hash = [All_shingle_len+1] * 100\n",
    "    for i in range(len(line[1])):\n",
    "        for j in range(100):\n",
    "            tmp[j] = (((j+2)*line[1][i] + 3*j ) % All_shingle_len)\n",
    "            if tmp[j] < real_hash[j]:\n",
    "                real_hash[j] = tmp[j]\n",
    "    # return documents as key hash_value of 100 hash function as value(list)\n",
    "    return line[0], real_hash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do the min hash\n",
    "All_shingle_len = len(All_shingle)\n",
    "Shingle_docs_RDD = Shingle_docs_RDD.map(mapper_hash)\n",
    "tmp_matrix = Shingle_docs_RDD\n",
    "#print(Shingle_docs_RDD.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mapper_LSH(line):\n",
    "    bucket = 0\n",
    "    return_list = []\n",
    "    for i in range(band):\n",
    "        bucket = 3*line[1][i]+11*line[1][i+1]+3\n",
    "        return_list.append(((i, bucket), [line[0]]))\n",
    "    return return_list\n",
    "\n",
    "def mapper_filter(line):\n",
    "    if len(line[1]) != 1:\n",
    "        return [(line[0], line[1])]\n",
    "    else:\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "band = 50\n",
    "Shingle_docs_RDD = Shingle_docs_RDD.flatMap(mapper_LSH).reduceByKey(lambda a, b:a+b).flatMap(mapper_filter)\n",
    "#print(Shingle_docs_RDD.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mapper_cal_sim(line):\n",
    "    return_list = []\n",
    "    for i in range(len(line[1])):\n",
    "        for j in range(len(line[1])):\n",
    "            if j >= i:\n",
    "                break\n",
    "            cnt = 0\n",
    "            for k in range(100):\n",
    "                if signature[line[1][i]-1][k] == signature[line[1][j]-1][k]:\n",
    "                    cnt+=1\n",
    "            return_list.append(((line[1][i], line[1][j]), cnt/100))\n",
    "    return return_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mapper_adjust_KV(line):\n",
    "    return line[1], (line[0][0], line[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_matrix = tmp_matrix.take(101)\n",
    "signature = [0]*101\n",
    "for i in range(101):\n",
    "    signature[i] = tmp_matrix[i][1]\n",
    "Shingle_docs_RDD = Shingle_docs_RDD.flatMap(mapper_cal_sim).reduceByKey(lambda a, b: a).map(mapper_adjust_KV).sortByKey(ascending = False)\n",
    "#print(Shingle_docs_RDD.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20,12): 1.0\n",
      "(84,52): 1.0\n",
      "(35,30): 0.79\n",
      "(49,47): 0.72\n",
      "(88,49): 0.67\n",
      "(88,47): 0.5\n",
      "(38,23): 0.45\n",
      "(49,48): 0.36\n",
      "(40,14): 0.35\n",
      "(48,47): 0.25\n"
     ]
    }
   ],
   "source": [
    "output = Shingle_docs_RDD.collect()\n",
    "outfile = open(\"Output.txt\", \"w\")\n",
    "for i in range(10):\n",
    "    print(\"(\" + str(output[i][1][0]) + ',' + str(output[i][1][1]) + \"): \"+ str(output[i][0]))\n",
    "    outfile.write(\"(\" + str(output[i][1][0]) + ',' + str(output[i][1][1]) + \"): \"+ str(output[i][0]))\n",
    "    outfile.write(\"\\n\")\n",
    "outfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 107062321 LSH Report\n",
    "\n",
    "### mapper_split\n",
    "在這個mapper裡面，我做的事情就是將每個word split開，由於有可能會split出空字串\n",
    "故我先判斷這個word是不是空字串，如果不是，就再判斷他的開頭是不是數字或英文字母\n",
    "如果是的話我就會把他留下\n",
    "\n",
    "### mapper_combine3\n",
    "在這個mapper裡，我做的事情就是要把一個一個word組成三個一組的3-shingle\n",
    "所以我把文字第i個文字與i+1, i+2個文字中間用一個空個連起來\n",
    "並且將這個string丟進一個binascii.crc32()的function裡\n",
    "在這裡面他可以幫我將這組string hash成一個integer\n",
    "達到講義教我們的，要把long shingle hash成4 bytes的integer\n",
    "\n",
    "最後我再將重複出現的shingle拿掉\n",
    "所以最後回傳回去的shingle set中，不會有重複的shingle\n",
    "回傳後我會再用reduceByKey(lambda a, b: a+b) 將docment再次可以接上自己的全部shingle\n",
    "\n",
    "### After shingle\n",
    "將所有document的shingle拼在一起後\n",
    "我會先把那個set sort過，因為後面要讓大家查index\n",
    "\n",
    "### mapper_into_all_index\n",
    "由於每個docment要知道自己這個shingle在 所有docments一起組成的shingle set中的index\n",
    "等等做minhash的時候，才能得到signature\n",
    "故這邊將每個docment的shingle丟進來，查找index後\n",
    "組成一個 (docment, index) 的 KV pair回傳，並用flatMap攤開，並且用reduceByKey\n",
    "使document各自後面就可以接著自己的shingle的index\n",
    "\n",
    "### mapper_hash\n",
    "在這個mapper裡，做的事情就是min-hash\n",
    "我選擇用的100個random hash function的通式為 \n",
    "    (((j+2)*line[1][i] + 3*j ) % All_shingle_len)\n",
    "其中 0<=j<=99，i為document中的shingle數量 並且最後mod所有document加起來的總shingle數\n",
    "故bucket為所有document的總shingle數\n",
    "\n",
    "### mapper_LSH\n",
    "在這邊要做的就是LSH，故要選擇一個hash function，足夠像的人才會被hash進同一個bucket\n",
    "而我的hash function為3*line[1][i]+11*line[1][i+1]+3\n",
    "\n",
    "其中line[1][i]與line[1][i+1]代表的是同一band中的兩個row的signature\n",
    "最後回傳((第幾個band, hash到的bucket), document)\n",
    "並且用reduceByKey(lambda a, b: a+b)將在同一band且被hash到同樣bucket的element放在一起\n",
    "\n",
    "### mapper_filter\n",
    "在這個mapper裡，我們要做的就是找出candidate pair\n",
    "所以將若一個bucket中，只有一個element的bucket，我們要將這個bucket丟掉\n",
    "也就是裡面的pair就不能成為candidate pair\n",
    "\n",
    "### mapper_cal_sim\n",
    "這個mapper就是要算candidate pair的document的similarity是多少\n",
    "所以就拿出他們signature的matrix來比對\n",
    "若他們某一row的signature一樣，就把一個cnt+=1\n",
    "等到掃完所有的row了，就建一個新的KV pair ((doc1, doc2), similarity)\n",
    "回傳後，由於可能會有重複的(doc1, doc2)，故用reduceByKey(lambda a, b: a)\n",
    "\n",
    "### mapper_adjust_KV\n",
    "最後為了sort similarity，所以我要把similarity放到key\n",
    "故這個mapper只是將 similarity放到key以便sort\n",
    "然後把(doc1, doc2)放到value的位置"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
